{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc955b2-6a4a-4e39-8fd8-bcd672097152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15212354-fd13-4673-b03d-8d273cbe8f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install transformers==4.31 \n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a66263-64d2-4793-a966-2b2b88e24e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\" \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a79cfd-e5b2-4ea1-8d5c-0f30f94a8abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c5afb3-a1e7-4562-a4e1-08dc8e9ea032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "def mystream(user_prompt,model):\n",
    "    model.config.use_cache = True\n",
    "    model.eval()\n",
    "    runtimeFlag = \"cuda:0\"\n",
    "    system_prompt = 'You are a helpful assistant that provides accurate and concise responses'\n",
    "\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "    prompt = f\"{B_INST} {B_SYS}{system_prompt.strip()}{E_SYS}{user_prompt.strip()} {E_INST}\\n\\n\"\n",
    "\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
    "\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "\n",
    "    # Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
    "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3670f24-1d39-4737-baa2-43887ea61793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkmodel(model_id,prompt):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "    \n",
    "    #generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    #generator(\"What is Aristotle's approach to logic?\")\n",
    "\n",
    "    mystream(prompt,model)\n",
    "\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe83504f-bc53-4bd9-b55d-a660ec3413f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################TRAIN##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d72a4e-ae7e-4a37-b8a0-a8738122b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dabada-d8c9-4991-8fb9-bfe59e43ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a353d2ed-52bd-4557-a001-a07e3cdae10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132c63e-1bdf-4a92-af1d-ff196f558772",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    # target_modules=[\"query_key_value\"],\n",
    "    target_modules=[\"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\"], #For Llama models.\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17879cb6-fc4a-4a53-b3fb-d8ce84675ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"TimelyFormulation74/askaphil\")\n",
    "data = data.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b3c9af-d869-42c9-87c7-49c1af78f7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TRAIN:\n",
    "import transformers\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token # </s>\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=10,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # re-enable for inference\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454894bf-f63d-4f09-8b0f-00c2c079acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_model_name = model_id.split(\"/\")[-1]\n",
    "\n",
    "adapter_model = f\"TimelyFormulation74/{base_model_name}-fine-tuned-adapters\" \n",
    "new_model = f\"TimelyFormulation74/{base_model_name}-fine-tuned\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f183a2a-8d11-423f-990c-4f8b6abdc7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Save model adapters\n",
    "model.save_pretrained(adapter_model, push_to_hub=True, use_auth_token=True)\n",
    "model.push_to_hub(adapter_model, use_auth_token=True)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a93bde7-5feb-4464-9d6c-ae7ee01eb062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge model with adapter \n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map='cpu', trust_remote_code=True, torch_dtype=torch.float16)\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    adapter_model,\n",
    ")\n",
    "model = model.merge_and_unload()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7954c1-e638-4bdd-acfd-9e4814f45bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save final model and tokenizer\n",
    "model.push_to_hub(new_model, use_auth_token=True, max_shard_size=\"5GB\")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.push_to_hub(new_model, use_auth_token=True)\n",
    "\n",
    "del model,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093c3700-75b3-40f3-b386-a4ceeea7ca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"TimelyFormulation74/Llama-2-7b-chat-hf-fine-tuned\"\n",
    "#model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "userprompt=\"\"\n",
    "checkmodel(model_id,userprompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
